{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Classification using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading News Articles Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset = 'bbc'\n",
    "\n",
    "categories = []\n",
    "text = []\n",
    "labels = []\n",
    "\n",
    "for news_group in os.listdir(dataset):\n",
    "    categories.append(news_group)\n",
    "    article_path = os.path.join(dataset, news_group)\n",
    "    try:\n",
    "        for filename in os.listdir(article_path):\n",
    "            news_article = os.path.join(article_path, filename)\n",
    "            with open(news_article, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                text.append(file.read())\n",
    "                labels.append(news_group)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting news articles to numerical array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     3,     9,  1339],\n",
       "       [    0,     0,     0, ...,     5,  2045,  2369],\n",
       "       [    0,     0,     0, ...,     6,     1,   442],\n",
       "       ...,\n",
       "       [  605,    21,  2431, ...,   469,   142,   226],\n",
       "       [    0,     0,     0, ..., 15968,     6,  2788],\n",
       "       [   44,     4,    57, ...,    13,     1,   373]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing and Pad Sequencing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(text)\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "data = pad_sequences(sequences, maxlen=1000)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting news categories into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Lable to Arrays\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dataset into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data to train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building LSTM Model to classify news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 7s 95ms/step - loss: 2.2423 - accuracy: 0.2102 - val_loss: 1.6289 - val_accuracy: 0.2670\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 2s 70ms/step - loss: 1.6201 - accuracy: 0.2210 - val_loss: 1.5912 - val_accuracy: 0.2670\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 2s 70ms/step - loss: 1.5862 - accuracy: 0.2487 - val_loss: 1.5907 - val_accuracy: 0.2216\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 2s 69ms/step - loss: 1.5175 - accuracy: 0.3580 - val_loss: 1.5439 - val_accuracy: 0.4091\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 2s 68ms/step - loss: 1.2758 - accuracy: 0.6856 - val_loss: 1.3624 - val_accuracy: 0.5455\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 2s 69ms/step - loss: 0.7302 - accuracy: 0.8681 - val_loss: 0.6871 - val_accuracy: 0.7727\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 2s 68ms/step - loss: 0.3468 - accuracy: 0.9331 - val_loss: 0.6468 - val_accuracy: 0.7727\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 2s 68ms/step - loss: 0.1840 - accuracy: 0.9684 - val_loss: 0.5414 - val_accuracy: 0.8068\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 2s 69ms/step - loss: 0.0599 - accuracy: 0.9931 - val_loss: 0.4448 - val_accuracy: 0.8636\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 2s 67ms/step - loss: 0.0359 - accuracy: 0.9962 - val_loss: 0.4631 - val_accuracy: 0.8580\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 2s 65ms/step - loss: 0.0428 - accuracy: 0.9943 - val_loss: 0.5163 - val_accuracy: 0.8352\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 2s 71ms/step - loss: 0.0169 - accuracy: 0.9994 - val_loss: 0.5634 - val_accuracy: 0.8182\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 2s 67ms/step - loss: 0.0186 - accuracy: 0.9975 - val_loss: 0.5621 - val_accuracy: 0.8011\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 2s 67ms/step - loss: 0.0129 - accuracy: 0.9987 - val_loss: 0.4765 - val_accuracy: 0.8523\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 2s 67ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.4465 - val_accuracy: 0.8580\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 2s 67ms/step - loss: 0.0113 - accuracy: 0.9987 - val_loss: 0.4872 - val_accuracy: 0.8523\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 2s 69ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.4430 - val_accuracy: 0.8636\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 2s 68ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3967 - val_accuracy: 0.8807\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 2s 70ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.3763 - val_accuracy: 0.8864\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 2s 65ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.3885 - val_accuracy: 0.8977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15533556610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building LSTM Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=1000))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training Model\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.13%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {scores[1]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Prediction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 375ms/step\n",
      "\n",
      "Article: \n",
      "S Korean credit card firm rescued\n",
      "\n",
      "South Korea's largest credit card firm has averted liquidation following a one trillion won ($960m; Â£499m) bail-out.\n",
      "\n",
      "LG Card had been threatened with collapse because of its huge debts but the firm's creditors and its former parent have stepped in to rescue it. A consortium of creditors and LG Group, a family owned conglomerate, have each put up $480m to stabilise the firm. LG Card has seven million customers and its collapse would have sent shockwaves through the country's economy.\n",
      "\n",
      "The firm's creditors - which own 99% of LG Card - have been trying to agree a deal to secure its future for several weeks. They took control of the company in January when it avoided bankruptcy only through a $4.5bn bail-out.\n",
      "\n",
      "They had threatened to delist the company, a move which would have triggered massive debt redemptions and forced the company into bankruptcy, unless agreement was reached on its future funding. \"LG Card will not need any more financial aid after this,\" Laah Chong-gyu, executive director of Korea Development Bank - one of the firm's creditors - said.\n",
      "\n",
      "The agreement will see some 12 trillion won of debt converted into equity. \"The purpose of the capital injection is to avoid delisting and the goal will be met,\" David Kim, an analyst at Sejong Securities, told Reuters. South Korea's consumer credit market has been slowly recovering from a crisis in 2002 when a credit bubble burst and millions of consumers fell behind on their debt repayments. LG Card returned to profit in September but needed further capital to avoid being thrown off the market. South Korea's stock exchange can delist any firm if its debt exceeds its assets two years running.\n",
      "\n",
      "\n",
      "Predicted Category: business\n"
     ]
    }
   ],
   "source": [
    "# Prediction Example\n",
    "import numpy as np\n",
    "\n",
    "test_article_index = np.random.randint(len(X_test))\n",
    "test_article = X_test[test_article_index]\n",
    "test_article_text = text[test_article_index]\n",
    "tokens = tokenizer.texts_to_sequences([test_article_text])\n",
    "test_article_processed = pad_sequences(tokens, maxlen=1000)\n",
    "\n",
    "predicted_category_index = model.predict(test_article_processed).argmax(axis=-1)\n",
    "predicted_category_name = categories[predicted_category_index[0]]\n",
    "\n",
    "print(f'\\nArticle: \\n{test_article_text}')\n",
    "print(f'\\nPredicted Category: {predicted_category_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put your own article to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('file_path', 'r', encoding='utf-8') as file:\n",
    "#     test_article = file.read()\n",
    "\n",
    "# tokenized = tokenizer.texts_to_sequences([test_article])\n",
    "# processed = pad_sequences(tokenized, maxlen=1000)\n",
    "\n",
    "# predicted_category_index = model.predict(processed).argmax(axis=-1)\n",
    "# predicted_category_name = categories[predicted_category_index[0]]\n",
    "\n",
    "# print(f'\\nArticle: \\n{test_article}')\n",
    "# print(f'\\nPredicted Category: {predicted_category_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
